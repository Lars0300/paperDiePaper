The algorithm loops over all 3 representation and gives back samples denoted as 
$A_O, A_F$ and $A_D$ respectively.
All together then are used as the samples for the TSF algorithm $A^* = A_O \cup A_F \cup A_D$ %TODO: ref
Each Set $A_R$ is generated by recursively searching for class discriminative 
interval features across multiple random partitions of the input data.
Let $X \in \mathbb{R}^{n \times m}$ be the dataset of $n$ univariate time series of length
$m$ and let $y\in \{1, \dots, c\}^n$ be the vector for class labels.
For every interval feature $f_k$ %TODO: ref
the algorithm select a random split index $u\in \{1,\dots, m-1 \}$ to divide each time series into a 
left and right sub-series. $X_L\in \mathbb{R}^{n \times u}, X_R \in \mathbb{R}^{n \times (m - u)}$
This randomized partitioning is repeated a fixed number of times to increase diversity of the search space. 
For each representation and aggregation function, the algorithm then applies a recursive supervised function on both halves via
SupervisedSearch($X_L$, $f_k$, $y$, $f_r= \text{Fisher score}$, $A={}$) and SupervisedSearch($X_L$, $f_k$, $y$, $f_r = \text{Fisher score}$, $A={}$)

This function works as follows: it recursively bisects the interval, evaluates the two resulting sub-intervals using the aggregation function $f_k$, 
and scores the resulting feature vector with the ranking metric $f_r$.
The half with the higher score is retained in $A$ and further partitioned, while the other is discarded.
The termination of the function is when the interval length falls below 2.

Each path gives one interval feature, which is defined by its start and end indices and the function used. 
Since the loops are constant and the procedure follows a binary partitioning pattern,
it returns $O(\log m)$ features per call. These features are collected into $A_R$ and ultimately passed into the 
tree classifier, which selects final splits during training.


\begin{algorithm}[H]
\caption{SupervisedSearch}
\begin{algorithmic}[1]
\Require $X' \in \mathbb{R}^{n \times m'}$: time series set, $y \in \{1,\dots,c\}^n$: class labels,\\
\hspace{1.6em} $f$: aggregation function, $f_r$: feature ranking metric, $\bar{A}$: set of candidate intervals
\Ensure Updated set $\bar{A}$ of discriminatory interval features

\If{$m' < 2$}
    \State \Return $\bar{A}$
\Else
    \State $a_L \gets f(X', 1, m'/2)$
    \State $a_R \gets f(X', m'/2, m')$
    \State $score_L \gets f_r(a_L, y)$
    \State $score_R \gets f_r(a_R, y)$
    \If{$score_L \geq score_R$}
        \State $\bar{A} \gets \bar{A} \cup \{a_L\}$
        \State \Call{SupervisedSearch}{$X'[:, 1:m'/2]$, $y$, $f$, $f_r$, $\bar{A}$}
    \Else
        \State $\bar{A} \gets \bar{A} \cup \{a_R\}$
        \State \Call{SupervisedSearch}{$X'[:, m'/2:m']$, $y$, $f$, $f_r$, $\bar{A}$}
    \EndIf
\EndIf
\State \Return $\bar{A}$
\end{algorithmic}
\end{algorithm}
